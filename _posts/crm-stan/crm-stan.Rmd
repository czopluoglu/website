---
title: "Measuring Oral Reading Fluency: A Case for Samejima's Continuous Response Model"
description: |
  I pitched the idea of using Samejima's Continuous Response Model (CRM) to measure the Oral Reading Fluency (ORF) a while ago when I published a paper in 2012. In that paper, I used an ORF dataset from the Minneapolis Public Schools District (MPS) as a real data example. Since then, I don't think anybody has bought the idea of using CRM to measure ORF, so here I am trying one more time with some accessible R code.

author:
  - name: Cengiz Zopluoglu
    affiliation: University of Miami
    
date: 12-31-2019

categories:
  - item response theory
  - continuous response model
  - Stan
  - R
  - "2019"
  
output:
  distill::distill_article:
    self_contained: true
    highlight_downlit: false

preview: images/image.png
---

<style>

body {
text-align: justify}

</style>

```{r echo = FALSE, eval=TRUE}

library(knitr)
# the default output hook
hook_output = knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
    if (!is.null(n <- options$out.lines)) {
        x = knitr:::split_lines(x)
        if (length(x) > n) {
            # truncate the output
            x = c(head(x, n), "....\n")
        }
        x = paste(x, collapse = "\n")  # paste first n lines together
    }
    hook_output(x, options)
})

opts_chunk$set(out.lines = 40)
opts_chunk$set(width = 40)

```

I pitched the idea of using Samejima's [Continuous Response Model (CRM)](https://link.springer.com/article/10.1007/BF02291114) to measure the [Oral Reading Fluency (ORF)](https://council-for-learning-disabilities.org/what-is-oral-reading-fluency-verbal-reading-proficiency) a while ago when I published [a paper](https://link.springer.com/article/10.3758/s13428-012-0229-6) as a graduate student. In that paper, I used an ORF dataset from the Minneapolis Public Schools Disctrict (MPS) as a real data example. Since then, I don't think anybody has bought the idea for using CRM to measure ORF, so here I am trying one more time with some accessible R code.

The ORF dataset from MPS was the only reason I was interested in CRM at all. During my second year in graduate school, I had a chance to complete a semester-long internship in MPS. The task I was assigned was to use IRT for scaling an ORF dataset they had. The dataset had  three variables representing the number of words read correctly per minute for three different reading passages. There were 2594 students. Below is a little information about the dataset.  

```{r echo = FALSE, eval=TRUE}

require(foreign)

d <- read.spss("C:/Users/cengiz/Documents/ORA.spring.2008.without ID.sav",
          to.data.frame=TRUE)


wpm <- na.omit(d[,c(1,6,11)])

options(width = 100)

```


```{r echo = TRUE, eval=TRUE}

str(wpm)

head(wpm)

require(psych)

describe(wpm)[,c("n","mean","sd","min","max","skew","kurtosis")]

cor(wpm)

```


At the time, I had taken an IRT course and knew a little bit about dichotomous and polytomous IRT models. It was a challenge because the measurement outcomes were kind of continuous. As I didn't know much about CRM at that time and how to deal with this, I created discrete categories for each variable such that Level 1 (0-50 words), Level 2(51-100 words), Level 3 (101-150 words), Level 4 (151-200 words), and Level 5 (201-250 words). Then, I fit polytomous IRT models and did usual IRT modeling based on the best fitting model. I completed the internship but I was not satisfied as there was so much information thrown away by discretizing. That's how I ended up exploring and discovering CRM and started working on it. I first briefly introduce the model before demonstrating how to use it for ORF data.

## Continuous Response Model

Below is the reparameterized version of the model by [Wang and Zeng (1998)](https://journals.sagepub.com/doi/abs/10.1177/014662169802200402). In this model, the probability of an individual with a latent parameter $\theta_i$ obtaining a score of $x$ or higher on an item with three parameters ($a$, $b$, and $\alpha$) is specified as,

$$P_{ij}(X\geq x|\theta_{i},a_j,b_j,\alpha_j)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{v}{e^{-\frac{t^2}{2}}dt}$$

$$v=a_j\times (\theta_i-b_j-\frac{1}{\alpha_j}ln\frac{x}{k-x})$$, 
where $k_j$ is the maximum possible score for the item. The parameter $a$ and $b$ have a very similar interpretation as in the 2PL IRT model. The $a$ parameter controls the steepness, and the $b$ parameter controls the location of the operating characteristic curves. There is an extra $\alpha$ parameter called "scaling parameter," and it controls the distance among the curves. 

Let's create some plots to see it better. Suppose there is an item with ten categories, and this item has the following parameters: $a=1$, $b=0$, and $\alpha=1$.The figure below displays how the operating characteristic curves look like for this item. Note that there are nine lines in the plot indicating $P(X\geq 1)$, $P(X\geq 2)$, $P(X\geq 3)$, ..., $P(X\geq 9)$.


```{r echo = TRUE, eval=TRUE}

 theta <- seq(-4,4,.01)

 a     = 1
 b     = 0
 alpha = 1

 x    <- 1
 k    <- 10
 v    <- a*(theta-b-(1/alpha)*log(x/(k-x)))
 plot(theta,pnorm(v,0,1),type="l",xlim=c(-4,4),ylim=c(0,1),
      main=expression(paste("a=1, b=0, ",alpha,"=1")),
      xlab=expression(theta),ylab="Cumulative Probability")

 for(x in 2:(k-1)){
	 v    <- a*(theta-b-(1/alpha)*log(x/(k-x)))
 	 points(theta,pnorm(v,0,1),type="l")
 }

```

Now, let's keep everything the same but increase the value of the parameter $a$ to 3, and we see how steepness changes.

```{r echo = TRUE, eval=TRUE}

 theta <- seq(-4,4,.01)

 a     = 3
 b     = 0
 alpha = 1

 x    <- 1
 k    <- 10
 v    <- a*(theta-b-(1/alpha)*log(x/(k-x)))
 plot(theta,pnorm(v,0,1),type="l",xlim=c(-4,4),ylim=c(0,1),
      main=expression(paste("a=3, b=0, ",alpha,"=1")),
      xlab=expression(theta),ylab="Cumulative Probability")

 for(x in 2:(k-1)){
	 v    <- a*(theta-b-(1/alpha)*log(x/(k-x)))
 	 points(theta,pnorm(v,0,1),type="l")
 }

```


Or, we can keep everything the same but change the value of the parameter $b$ to -2, and we see the curves shift to left.

```{r echo = TRUE, eval=TRUE}

 theta <- seq(-4,4,.01)

 a     = 1
 b     = -2
 alpha = 1

 x    <- 1
 k    <- 10
 v    <- a*(theta-b-(1/alpha)*log(x/(k-x)))
 plot(theta,pnorm(v,0,1),type="l",xlim=c(-4,4),ylim=c(0,1),
      main=expression(paste("a=1, b=-2, ",alpha,"=1")),
      xlab=expression(theta),ylab="Cumulative Probability")

 for(x in 2:(k-1)){
	 v    <- a*(theta-b-(1/alpha)*log(x/(k-x)))
 	 points(theta,pnorm(v,0,1),type="l")
 }

```


Lastly, let's change the value of the parameter $\alpha$ to 4, and we see the distance among curves gets smaller.


```{r echo = TRUE, eval=TRUE}

 theta <- seq(-4,4,.01)

 a     = 1
 b     = 0
 alpha = 4

 x    <- 1
 k    <- 10
 v    <- a*(theta-b-(1/alpha)*log(x/(k-x)))
 plot(theta,pnorm(v,0,1),type="l",xlim=c(-4,4),ylim=c(0,1),
      main=expression(paste("a=1, b=0, ",alpha,"=4")),
      xlab=expression(theta),ylab="Cumulative Probability")

 for(x in 2:(k-1)){
	 v    <- a*(theta-b-(1/alpha)*log(x/(k-x)))
 	 points(theta,pnorm(v,0,1),type="l")
 }

```

For those who are familiar with the Graded Response Model (GRM) may see the difference. In GRM, we estimate a step parameter for each response category, and these step parameters control the location of the operating characteristic curves and hence the distance among the curves at the same time. So, if there is an item with ten categories, GRM estimates nine step parameters. Instead, CRM estimates only two parameters ($b$ and $\alpha$) per item to control the location and distance among the curves regardless of the number of categories. So, CRM is a restricted version of GRM where we control the distance among the curves with only one parameter. If we think the $b$ parameter a kind of common difficulty parameter for an item, the term $$\frac{1}{\alpha_j}ln\frac{x}{k-x})$$ decides how much each score is away from that common difficulty after accounting for the distance between the score and the maximum possible score. 

Wang & Zeng (1998) also simplifies the equation by transforming the observed response (this is like a logit transformation for the observed responses), $$Z=ln\frac{X}{k-X}$$ and provides the conditional PDF of Z as

$$f(z|\theta_i,a_j,b_j,\alpha_j)=\frac{a}{\alpha\sqrt{2\pi}}e^\frac{-(a_j(\theta_i-b_j-\frac{z_i}{\alpha_j}))^2}{2}$$, which is a normal density function with a mean of $\alpha_j(\theta_i - b_j)$ and standard deviation of $\alpha_j/a_j$.

## Formatting Data for Stan

There are other tools we can use to fit CRM. I will focus here fitting the model using Stan, but I also provide two other tools at the very end as supplemental material. Before introducing the Stan model syntax, I should first reformat data as this is the only way to run Stan with missing data. Although I don't have missing data in the ORF dataset used for this blog post, I force myself to use this procedure to get used to it.

```{r echo = TRUE, eval=TRUE}

# Wide format data (2594 x 3) with rows representing a different individual and
# columns represent a different outcome

head(wpm)
dim(wpm)


# add an arbitrary individual id variable

wpm$id <- 1:nrow(wpm)

# Convert the wide format data to long format (7782 x 3)
# This is like repeated measures data

wpm.long <- reshape(data      = wpm,
                  idvar       = "id",
                  varying     = list(colnames(wpm)[1:3]),
                  timevar     = "Item",
                  times       = 1:3,
                  v.names      = "score",
                  direction   = "long")


wpm.long <- wpm.long[order(wpm.long$id),]

head(wpm.long)
dim(wpm.long)

  # Each individual is represented in three rows, one for each item
  # There are extra two columns for item and person id


# The next line doesn't change anything for this case as there is no
# missing data but this is critical step if you have missing data 

wpm.long <- na.omit(wpm.long) 


# Transform the observed scores to logits(from X to Z in the above description)
# This assumes each reading passage has 250 words so the
# maximum possible score is 250

wpm.long$score <- log(wpm.long$score/(250-wpm.long$score))

head(wpm.long)

# Create each column in the long format data as separate vectors
# These are what we will feed to Stan as our data

id            <- wpm.long$id      
item          <- wpm.long$Item
score         <- wpm.long$score
n_obs         <- length(wpm.long)

```



## Stan Model Syntax


Below is the Stan model syntax for fitting CRM. I follow the guidelines in the Stan manual and imposes weakly-informative
prior on model parameters.


```{r echo = TRUE, eval=FALSE}

crm <- '

  data{
	int  J;                    //  number of items
	int  I;                    //  number of individuals
	int  n_obs;                //  number of observed responses
  int  item[n_obs];          //  item id
  int  id[n_obs];            //  person id
  real Y[n_obs];             //  vector of transformed outcome
 }

  parameters {
  
    vector[J] b;                 // vector of b parameters forJ items
      real mu_b;                 // mean of the b parameters
      real<lower=0> sigma_b;     // standard dev. of the b parameters

    vector<lower=0>[J] a;       // vector of a parameters for J items
      real mu_a;                 // mean of the a parameters
      real<lower=0> sigma_a;     // standard deviation of the a parameters
      
    vector<lower=0>[J] alpha;   // vector of alpha parameters for J items     
      real mu_alpha;             // mean of alpha parameters
      real<lower=0> sigma_alpha; // standard deviation of alpha parameters
       
    vector[I] theta;             // vector of theta parameters for I individuals      
  }

  model{

     mu_b     ~ normal(0,5);
     sigma_b  ~ cauchy(0,2.5);
         b    ~ normal(mu_b,sigma_b);

     mu_a    ~ normal(0,5);
     sigma_a ~ cauchy(0,2.5);
         a   ~ normal(mu_a,sigma_a);
      
     mu_alpha ~ normal(0,5);
     sigma_alpha ~ cauchy(0,2.5);
         alpha   ~ normal(mu_alpha,sigma_alpha);

     theta   ~ normal(0,1);      // The mean and variance of theta is fixed to 0 and 1
                                 // for model identification

      for(i in 1:n_obs) {
        Y[i] ~ normal(alpha[item[i]]*(theta[id[i]]-b[item[i]]),alpha[item[i]]/a[item[i]]);
       }
   }
'

```


## Fitting the Model

The code below fits the model with four chains and 50,000 iterations for each chain. By default, the first 50% of the iterations are used as warm-up and the remaining 50% of the iterations used for inference.

```{r echo = TRUE, eval=FALSE}

require(rstan)

data <- list(I     = 2594,
             J     = 3,
             n_obs = 7782,
             id    = id,
             item  = item,
             Y     = score)

fit_crm <- stan(model_code = crm, 
                data       = data, 
                iter       = 50000,
                chains     = 4,
                control    = list(max_treedepth = 15,adapt_delta=0.9))

```


```{r echo = FALSE, eval=TRUE}

load("C:/Users/cengiz/Documents/crm_stan.Rdata")
require(rstan)
require(psych)
```

```{r echo = TRUE, eval=TRUE}

print(get_elapsed_time(fit_crm))

sum(get_elapsed_time(fit_crm))/3600    # how many hours it took to run the model with the 
                                       # above settings
```


## Model Convergence

```{r echo = TRUE, eval=TRUE}

print(fit_crm, 
      pars = c("b","mu_b","sigma_b",
               "a","mu_a","sigma_a",
               "alpha","mu_alpha","sigma_alpha"), 
      probs = c(0.025,0.975), 
      digits = 3)


```

It seems that we have convergence, and all chains yield consistent results as all $\hat{R}$ values are less than 1.01, The figures below are the posterior distributions for all these parameters. They all look fine.

```{r echo = TRUE, eval=TRUE}

plot(fit_crm, 
     plotfun = "hist", 
     pars = c("b[1]","b[2]","b[3]",
              "a[1]","a[2]","a[3]",
              "alpha[1]","alpha[2]","alpha[3]"), 
     inc_warmup = FALSE)


```


I also used the recommendations provided [at this link](https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html) to check some other things (seemed important). None of them indicated any red flag about the model convergence.


```{r echo = TRUE, eval=TRUE}

source("https://raw.githubusercontent.com/betanalpha/knitr_case_studies/master/qr_regression/stan_utility.R")

check_treedepth(fit_crm,15)

check_energy(fit_crm)

check_div(fit_crm)
```


# Parameter Interpretation

```{r echo = TRUE, eval=TRUE}

b     <- summary(fit_crm)$summary[1:3,1]

b

a     <- summary(fit_crm)$summary[6:8,1]

a

alpha <- summary(fit_crm)$summary[11:13,1]

alpha

thetas = summary(fit_crm)$summary[16:2609,1]

```

The $b$ parameters for three reading passages in this dataset were `r round(b[1],2)`, `r round(b[2],2)`, `r round(b[3],2)`. I think it is fair to call them "reading passage difficulty" in this context. Given that we fix the mean and variance of the latent person parameter ($\theta$) to 0 and 1, we can compare the passage difficulty relative to the levels of individuals. Note that $\theta$ represents here a latent variable indicating oral reading fluency. So, these three reading passages seem very difficult compared to the ORF levels of these individuals.

The $a$ parameters were `r round(a[1],2)`, `r round(a[2],2)`, `r round(a[3],2)`. We can call them "reading passage discrimination" parameters. So, these reading passages are highly informative about the latent construct being measured. These very high discrimination parameters are not very surprising, given that the correlations among the three variables were 0.94, 0.94, and 0.96.

The $\alpha$ parameters were `r round(alpha[1],2)`, `r round(alpha[2],2)`, `r round(alpha[3],2)`. I think it would be wise to go back and fix all the $\alpha$ parameters to 1 in the model and then re-fit, but I will not do it. Note that the $\alpha$ scales the distance between the response categories.

We can plot the operating characteristic curves, but it would be too messy for all 250 score categories, However, I can draw it for  $P(X\geq 50)$, $P(X\geq 100)$, $P(X\geq 150)$, ..., $P(X\geq 200)$. Below is the plot for Item 1.

```{r echo = TRUE, eval=TRUE}

 theta <- seq(-4,4,.01)

 a1     = 3.20
 b1     = 1.52
 alpha1 = 0.83

 x    <- 50
 k    <- 250
 v    <- a1*(theta-b1-(1/alpha1)*log(x/(k-x)))
 
 plot(theta,pnorm(v,0,1),type="l",xlim=c(-2,4),ylim=c(0,1),
      main=expression(paste("a=3.2, b=1.5, ",alpha,"= 0.8")),
      xlab=expression(theta),ylab="Cumulative Probability")

 for(x in c(100,150,200)){
	 v    <- a1*(theta-b1-(1/alpha1)*log(x/(k-x)))
 	 points(theta,pnorm(v,0,1),type="l")
 }
 
 abline(h=.5,lty=2)
 
 text(-0.2,.8,"P(X>50)",cex=0.8)
 text(0.95,.8,"P(X>100)",cex=0.8)
 text(1.95,.8,"P(X>150)",cex=0.8)
 text(3.1,.8,"P(X>200)",cex=0.8)
 
```


Let's check the latent person parameter estimates.

```{r echo = TRUE, eval=TRUE}

thetas = summary(fit_crm)$summary[16:2609,1]

psych::describe(thetas)

hist(thetas)

```

As expected, the estimates have a mean of 0 and a standard deviation of .99. 

Lower estimates indicate lower levels of oral reading fluency, and higher estimates indicate higher levels of oral reading fluency.

We can have a better understanding if we check a few individuals.


```{r echo = TRUE, eval=TRUE}


low = order(thetas,decreasing=F)[1:5]
high = order(thetas,decreasing=T)[1:5]

# The individuals with the highest oral reading fluency and their reading scores

cbind(wpm[high,],thetas[high])

# The individuals with the lowest oral reading fluency and their reading scores

cbind(wpm[low,],thetas[low])

```

For instance, the model provides an estimate of 3.11 for a student who had read 181 words, 224 words, and 220 words and an estimate of -2.28 for a student who had read 12 words, six words, and eight words in the three reading passages, respectively. We can check how the estimated theta is related to the scores from each reading passage for all examinees.


```{r echo = TRUE, eval=TRUE}

plot(thetas,wpm[,1],xlab=expression(theta),ylab="Word per minute",main="Reading Passage 1")

plot(thetas,wpm[,2],xlab=expression(theta),ylab="Word per minute",main="Reading Passage 2")

plot(thetas,wpm[,3],xlab=expression(theta),ylab="Word per minute",main="Reading Passage 3")

```

It is not surprising to see that there is a strong relationship because of the high discrimination parameters. Note that the relationship is not linear as we model the relationship between $\theta$ and $ln\frac{x}{k-x}$

# Model Fit Assessment

The first thing I would like to look at is how well the model predicts the observed ORF scores. Treating the item and person parameter estimates as known, we can calculate a predicted score on each item for each individual as well as a standardized residual.

Note that I mentioned before that the conditional PDF of Z is a normal density function with a mean of $\alpha_j(\theta_i - b_j)$ and standard deviation of $\alpha_j/a_j$, where Z is defined as $$ln\frac{X}{k-X}$$.


```{r echo = TRUE, eval=TRUE}

# Remember the data with observed outcomes

head(wpm.long)


# For each observation, compute the model prediction

wpm.long$pred <- NA
wpm.long$sres <- NA

for(i in 1:nrow(wpm.long)){
  
      wpm.long[i,]$pred = alpha[wpm.long[i,2]]*(thetas[wpm.long[i,1]]-b[wpm.long[i,2]])
  
      wpm.long[i,]$sres = (wpm.long[i,]$score-wpm.long[i,]$pred)/(alpha[wpm.long[i,2]]/a[wpm.long[i,2]])
  
}

head(wpm.long)

```

Note that the observed values and predicted values are on a logit metric because of the transformation. Let's check the correlation between observed outcomes and model predicted outcomes for each item.

```{r echo = TRUE, eval=TRUE}

cor(wpm.long[wpm.long$Item==1,c("score","pred")])
plot(wpm.long[wpm.long$Item==1,c("score","pred")])


cor(wpm.long[wpm.long$Item==2,c("score","pred")])
plot(wpm.long[wpm.long$Item==2,c("score","pred")])

cor(wpm.long[wpm.long$Item==3,c("score","pred")])
plot(wpm.long[wpm.long$Item==3,c("score","pred")])
```

It looks like the model predicts the observed outcomes pretty well. 

We can also check if there is any systematic relationship between residuals and the latent variable. 

```{r echo = TRUE, eval=TRUE}

plot(thetas,wpm.long[wpm.long$Item==1,c("sres")])

plot(thetas,wpm.long[wpm.long$Item==2,c("sres")])

plot(thetas,wpm.long[wpm.long$Item==3,c("sres")])

```

There are some individuals with extreme residuals for each item; however, there doesn't seem to be any systematic pattern between residuals and the latent variable.

We can also check the distribution of standardized residuals within each item.


```{r echo = TRUE, eval=TRUE}

psych::describeBy(wpm.long$sres,wpm.long$Item)


hist(wpm.long[wpm.long$Item==1,]$sres)

hist(wpm.long[wpm.long$Item==2,]$sres)

hist(wpm.long[wpm.long$Item==3,]$sres)

```

There are some extreme residuals we may want to look deeper. So, let's filter the observations with a standardized residual outside of the range [-4,4] and look at their response pattern.


```{r echo = TRUE, eval=TRUE}

outliers = which(abs(wpm.long$sres)>4)

length(outliers)

wpm.long[outliers,]

outlier.ids <- unique(wpm.long[outliers,]$id)

outlier.ids

wpm[outlier.ids,]


```

Here, we can see there are 12 individuals with observations yielding extreme outliers. A quick examination of their score pattern reveals that these are typically individuals that scored low for one or two reading passages while scoring relatively much higher in other reading passage. So, for instance, individual 919 read 20 and 18 words for Reading Passage 1 and 3 while reading 221 words for Reading Passage 2. This is extremely unusual and may be due to data coding error, scoring error, low effort, or low motivation for some reason. It might be a good idea to flag these individuals with inconsistent (or unreliable) scores, remove them from the dataset, and re-fit the model without these individuals.

After removing the outliers, the standardized residuals follow an approximately normal distribution.

```{r echo = TRUE, eval=TRUE}

psych::describeBy(wpm.long[-outliers,]$sres,wpm.long[-outliers,]$Item)

```


A final check could be the residual correlations. As we assume local independence (and/or unidimensionality), the correlations among the reading scores should be ideally 0 once we account for the latent oral reading fluency parameter in the model.

```{r echo = TRUE, eval=TRUE}

cor(wpm.long[wpm.long$Item==1,c("sres")],wpm.long[wpm.long$Item==2,c("sres")])

cor(wpm.long[wpm.long$Item==1,c("sres")],wpm.long[wpm.long$Item==3,c("sres")])

cor(wpm.long[wpm.long$Item==2,c("sres")],wpm.long[wpm.long$Item==3,c("sres")])

```

The high residual correlations were the only thing I was not happy about this model. I don't know why the correlations among the residuals are unusually high. This suggests that there is still some variation left unexplained after accounting for one latent variable. This might suggest a second (latent) variable, but I don't think there is much I can do here to improve the model with only three items. I guess I'll just live with this.

## APPENDIX A: Alternative Ways of Fitting the Continuous Response Model

### 1. Heuristic Estimation through Linear Factor Model

The heuristic estimation is a method used by [Bejar (1977)](https://journals.sagepub.com/doi/abs/10.1177/014662167700100407) and also discussed in detail by [Ferrando(2002)](https://www.tandfonline.com/doi/abs/10.1207/S15327906MBR3704_05). It is very convenient as you can fit this model with any SEM software, so it is also easy to extend the model to multiple-group, multidimensional, or mixture cases, [see this paper I worked on for mixture extension of CRM](https://journals.sagepub.com/doi/full/10.1177/0013164419856663). 

In this approach, you can first transform the observed scores ($X$) using the equation $ln\frac{X}{k-X}$. Then, you can fit a simple linear factor model to the transformed scores. Finally, you put the model parameter estimates back to IRT metric. Below is a quick implementation in the lavaan package.

```{r echo = TRUE, eval=TRUE}

wpm.transformed <- log(wpm[,1:3]/(250-wpm[,1:3]))

head(wpm.transformed)

require(lavaan)


lfm <- 'f1 =~ NA*spas1wpm + l2*spas2wpm + l3*spas3wpm
        
        spas1wpm ~~ r1*spas1wpm   
        spas2wpm ~~ r2*spas2wpm
        spas3wpm ~~ r3*spas3wpm
        
        f1 ~~ 1*f1'

fit <- cfa(model  = lfm,
           data   = wpm.transformed)

parameterEstimates(fit)

intercepts <- colMeans(wpm.transformed)

intercepts

```

Once you estimate the parameters from the linear factor model, then you can transform them back to the IRT metric. 

The unstandardized factor loadings (0.824, 1.008, and 0.996) correspond to the $\alpha$ parameters in CRM. You can check back the estimates from Stan, and you will see that they are identical. 

If you divide the unstandardized factor loadings by residual standard deviation, you will get the discrimination parameters ($a$) of CRM. 

$$\frac{0.824}{\sqrt{0.066}} = 3.21$$
$$\frac{1.008}{\sqrt{0.038}} = 5.18$$
$$\frac{0.996}{\sqrt{0.047}} = 4.59$$

Again, if you check the estimates for $a$ parameters we obtained from Stan, they are almost identical. 

Finally, if you divide the intercepts by the unstandardized factor loadings and then multiply the result by -1, that should give you the difficulty parameters of CRM ($b$).


$$-\frac{-1.253}{0.824} = 1.52$$
$$-\frac{-1.139}{1.008} = 1.13$$
$$-\frac{-1.265}{0.996} = 1.27$$

Similarly, these values are almost identical to $b$ parameters obtained from Stan fit.

Another bonus is that you can also extract the factor scores from the lavaan object for each individual, and these factor scores should be almost identical to the $\theta$ estimates obtained from Stan fit.


```{r echo = TRUE, eval=TRUE}

fscores <- lavPredict(fit)

psych::describe(fscores)

cor(thetas,fscores)

plot(thetas,fscores)

```


### 2, Full-information Marginal Maximum Likelihood Estimation

This approach is fully described in two papers, [Wang and Zeng (1998)](https://journals.sagepub.com/doi/abs/10.1177/014662169802200402) and [Shojima(2005)](https://www.jstage.jst.go.jp/article/etr/28/1-2/28_KJ00003899231/_pdf/-char/ja). Wang and Zeng (1998) is a classical implementation of EM algorithm. However, Shojima(2005) is a really interesting paper as he derives the closed-form equation for the marginal likelihood, so there is no integration needed for the E-step. When I was first interested in this model, I created an R package, [EstCRM](https://cran.r-project.org/web/packages/EstCRM/index.html), that implements both approaches. Below is a code using this package to fit the model.

```{r echo = TRUE, eval=TRUE}

require(EstCRM)

CRM <- EstCRMitem(wpm[,1:3], 
                  max.item=c(250,250,250), 
                  min.item=c(0,0,0),
                  max.EMCycle=500, 
                  converge=.01,
                  type="Wang&Zeng",
                  BFGS=TRUE)

CRM$param

```

Here it is! Again, we get almost identical estimates for item parameters. It is also possible to compute $\theta$ and compare them from the person parameter estimates from the previous two approaches (Stan and lavaan).

```{r echo = TRUE, eval=TRUE}


theta.CRM <- EstCRMperson(data     = wpm[,1:3],
                          ipar     = CRM$param,
                          max.item = c(250,250,250), 
                          min.item = c(0,0,0))

psych::describe(theta.CRM[[1]][,2])

cor(cbind(thetas,fscores,theta.CRM[[1]][,2]))

```
